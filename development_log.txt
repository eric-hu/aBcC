8/21/2015

Paired today.  Spent most of the time again trying to figure out why an
encoded, hashed value we built doesn't match what we've packet sniffed from a
known working bittorrent client.  Used a Ruby REPL to help us reason about some
encodings, but ran into inconsistencies that left us scratching our heads.

Concluded the pairing session by talking about forward progress and learning
about Clojure.  Figuring out differences in byte encodings feels less
gratifying than learning about other aspects of Clojure.

Things learned:
- Ruby represents bytes differently from Java (Java: signed ints, Ruby:
  unsigned ints (?))
- Ruby and Java may decode URLs differently (different byte arrays produced
  from the decoding the same string)
- in Clojure and Java, byte arrays show up as a "[B" when you query for their
  class
- byte arrays have a length _field_, which is called using ".-field" syntax in
  Clojure

Next steps/ideas:
- try to tease out differences in Ruby and Java URL decoding schemes.  Maybe
  try encoding the same string in both languages?
- google for Java bittorrent source code

6/26/2015

Paired today, working on this for the first time in months.  It's much easier
to carve out time for this when someone else is interested too.

We created a test out of the result of past packet-sniffing work on the
handshake protocol.  SHA1 encoded version of string is not mapping to what I
want.

Next steps:
- Try decoding and encoding "info" value from file, compare with original.  We
  expect it to be the same, it's possible that it's not.

3/23/2015

Encoding

It's surprising how details of only one side of a language or format are
thought about when writing a parser.  When I built the bencode decoder, I felt
like I thought through the bencode format to a fair depth.  Now that I'm
building the translation from some Clojure structures into bencode, I'm
realizing sublteties of some basic Clojure data types.  I also have some more
questions about the Bencode data types.  Some questions include:

- What should I do with Clojure floats?
  - These are unnecessary to support, since Bencode doesn't have floats
- What about Clojure character sequences?
  - Support for these might be desired, based on my experience with strings and
    character sequences being interchangeable.  For now, I'll say that it can
    be built when needed.
- Do Bencode hashes have to be sorted?
- How many ways are there to represent the same Bencode dictionary as a Clojure
  hash?
  - At least keyword and string based keys come to mind
- Does Clojure iterate over maps in any particular order?
- How should the bencode encoder handle hashmaps with colliding entries?

Slow and Steady

A part of me inside flares up with impatience about getting more feature
completion faster.  A conversation I had at work today reminded me that I
really believe in the value of slow and steady learning.  If I had to quantify
most of my developer knowledge, I would say that most of it was earned in small
pieces, not large chunks.

Stumbles

I stumbled around with lazy sequences and anonymous functions a bit today.  I
eventually figured out what I need, but I can see that I don't quite have the
intuition to pick up when an error is due to an unrealized sequence.
Similarly, I spent some time trying to get the syntax right for an anonmous
function.  When all was said and done, I had concerns about the quality of the
code I wrote, but that can be refactored for style another day.


3/9/2015

Spiking

I discovered my next step by spiking on the next minimal feature.  It's been a
while since I last integrated spiking into my normal feature development
workflow.  It's quite nice.

Validation

I was wrong about the minimum necessary parameters to connect to a tracker.
The only key-value pair required for connecting is "info_hash", which I
discovered by packet sniffing a third party tracker client.  I was then able to
strip down its query string, one argument at a time until only "info_hash" was
left.

Changing the value of info_hash to something invalid changed the tracker
response from a 200 request with a body to a 400 without one.  The key to
getting a proper response is to have a valid info_hash key-value pair.  It
would appear that the next step is choosing a strategy for generating a valid
info_hash.  I think I prefer approach 1 described from my last post, as it
is more robust against different torrent files and makes for a better learning
experience, which is what this project is all about.

Refactoring, organization

To date, I've put all functions in the core class of this project.  The next
feature I want to work on is bencode-encoding, which is related to
bencode-decoding.  This feels like as good a place as any to break these out
into a project library and learn about how to organize Clojure code a little
better.

Lein-ancient

Lein-ancient is the first piece of functionality that I consider basic for
dependency management not supported out of the box by Leiningen.  It's a plugin
that allows for dependencies to be checked.  Installation was pretty simple, as
was using it to update my profile dependencies.

1/24/2015

Re-encode?

In an interesting twist, one of the first steps in communicating with a tracker
is to send a SHA-1 encoding of the requested torrents' info hash from the
torrent metadata file.  That is, one is supposed to apply SHA-1 to the value of
the "info" key in the bencode dictionary, without decoding the value.

A couple of different approaches are discussed online:

1. Re-encode the value of "info" from the dictionary.
2. Read the remainder of the input file after "info" raw.

Approach 1 seems wasteful, however, that isn't an issue with correctness, just
efficiency.  Approach 2 seems efficient, but is making a gaping assumption that
the "info" key is the last key in the dictionary (otherwise it's unknown how
many terminator characters need to be discarded).

Most of my work tonight was trying to run through some of this work by hand,
trying to send the data over the wire to a torrent tracker.  I generally
received undescriptive error messages from the server.  Upon reviewing the
bittorrent spec, I see that there are several parameters that should be passed
in the request.  Some have the "optional" label, most do not.  It seems that
trying to get a successful request by passing one parameter at a time is a weak
strategy, as the error messages from the tracker did not return specifics about
what was invalid.

Wireshark:

I found a blog of someone else who went through a torrent client exercise.  Her
suggestion for doing this was to use a packet sniffer and another torrent
client as a reference.  Some part of me wanted to be purist about this whole
experience, but I see that eventually I'd have to talk to a tracker server that
I haven't built, at the very least.

Test Driving:

I took a non-test driven approach to get a first understanding of the problem
domain.  I can see that test driving is not always the easiest way to solve a
problem.  For exploring a new API, it's much less constraining to make requests
interactively to discover what my expectations are.

Next steps:

I think monitoring packets sent by a 3rd party client is a good first start.
It'll shorten the development feedback cycles to know how a valid
implementation calculates the info hash, so I can use the raw value instead of
a guess-and-check approach of checking my work.  Installing a packet sniffing
tool seems to be the next best use of my time.

1/18/2015

Focus:

I was only able to dedicate a couple of hours of focused time today, along with
a couple more hours of unfocused time.  Doing this while multi-tasking is not a
fruitful endeavor.

Simplify when debugging:

I picked up today where I left off previously--the bencode parser that I
believe to be working correctly was not parsing the Ubuntu iso torrent.  I
suspected it might have to do with file encoding, but didn't quite know how to
verify this.  I ended up googling for a small torrent example file to work off
of, and this pointed me in the right direction.

The smaller file was one I could open and fully contain in my head, and it
looked as I expected _in vim_.  It gave me a similar error when I loaded it up
in the Clojure REPL and tried parsing it.  However, I noticed that what I was
slurping from the file didn't match what I saw in vim.  This prompted me to
re-check my experience in vim, and I noticed that vim reported "[converted]"
when the file was opened.

I googled for this and apparently vim does some auto-conversion of files that
it opens.  It stores the target encoding in the ":set encoding" variable, and
the source encoding in ":set fileencoding" variable.  Bingo, latin1 is the
encoding of the smaller torrent file as well as the Ubuntu file.

Character encoding:

Apparently the world of character encoding is a big mess.  I found a Joel
Spokesky article about character encoding, which was comprehensive, and I
really hoped my problem didn't lay in having to autodetect character encoding
by examining bytes.  Thankfully, I got my test cases working without that, but
if I were making a production torrent client, then I would want to provide
automatic character encoding detection.


1/10/2015
Sloppiness:

As I look over my tests, I can't help but feel like my tests are quite sloppy.
It's kind of refreshing to find myself back in this this place, so unfamiliar.
It revises my awareness for where beginners come from, both in the programming
world and outside.

Leave it dirty:

The refactorings I did today were natural removals of duplicated code that
became apparent once all the list, dictionary, and full-file parsing functions
were written.  I remember first sitting down with this problem and thinking
long and hard about how to generalize the parsing so that no unnecessary code
would be written.  I didn't get very far with that approach.

There's a certain ease of refactoring that can emerge if one is willing to
leave code dirty, even knowing that there will be some duplicated effort.  Code
that is written and deleted before a feature is complete isn't wasted.  It's
scaffolding for the final product.


1/6/2015
Cleanliness: I can feel old habits getting in the way of learning.  I'm still
  sticking to my 80 character width coding hygiene, but it makes some line
  formatting awkward.  It raises questions about how to properly format "let"
  vectors when pairs need to be split up (i.e. key on one line, value on the
  other).

  Another part of me says that these curiosities ("what is well-formatted
  Clojure in this situation?") are part of the natural progression of learning.
  Instead of "obstructing" my learning, it can be said that I'm creating my own
  motivation for learning more, though I am deviating from the linear path of
  building out more features in this project.

Error in test case, unterminated dictionary: This has highlighted the
  utility of error checking on dictionaries and lists, which I've added to the
  desired features of this project.

Error in test case, invalid bencode string: This has happened a few times that
  I can recall now.  I add invalid bencode strings into the test suite and then
  try debugging errors that come from invalid data.  This is simply the cost of
  hand-rolling bencoded data for my test suite instead of using a tool.
  Unfortunately for bencoded strings, it appears that there's not much I could
  actually check in terms of error-handling.  Strings that are declared as too
  short or too long won't be known to be until the next parse attempt.  The
  exception to this is when a string is the last bencoded value and is declared
  to be longer than it actually is.  This seems like enough of an edge case
  that I won't explicitly check it in code.

  If this happens more, then it might be worthwhile to programmatically
  generate bencoded test strings, since the operation is actually quite simple.

"Declare" for mutual recursion: I recall looking for this feature while working
  on this project previously.  What prevented me from finding it before was
  that I lacked the vocabulary to know what to search for.  This problem has
  illuminated to me the nature of learning.  One's capacity to learn in a new
  domain is small because one's vocabulary in that domain is small.  There is
  apparently a snowball effect in learning ability, as the domain vocabulary
  grows large enough to form many meaningful connections.  "declare" was a real
  joy to discover.


1/5/2015
Arity-overloading: I defined my first arity-overloaded function today.  I had
mixed feelings while using it.  Part of it feels a little dirty, coming from an
OO world.  It feels like the function is violating the idea of SRP.  I reminded
myself that SRP was defined on class definitions, so it doesn't really apply to
functions.  This isn't to say that I believe functions can be arbitrarily
large, but the division of responsibility between an FP style and an OO style
looks different.

Default case: By the end of my work today, I felt like using arity-overloading
to handle default cases, when using recursion, was actually a clean way to
implement behavior.  There's an "if" that's rendered implicit that would have
to be explicit without arity-overloading.

Recur: Clojure apparently does not perform tail call optimizations for
recursion, except when "recur" is used.  "Recur" based recursion does not
consume additional stack frames, and is recommended for functions intended to
run on data with unknown bounds.  Changing a recursive function to use "recur"
means making the recursive call a tail call.  Doing this sometimes means
passing "wrap-up" work forward to the next call.

Accumulating output: This seems to be another clean case for using
arity-overloading.

Elegance (fear of inelegance): oftentimes I've had stare-at-the-screen moments
when I have a rough idea of what to do, but no idea of how to do it cleanly.
To this, I say that I want myself to continue with dirty implementations over
stagnating while trying to find a clean one.

Test-driving: I've been fairly happy with my sense of progress while using a
test-driving approach.  This hygiene practice has carried over cleanly from my
work and really helps me to keep a sane chunk of work in front of me--and
sometimes it points out that my assumptions are wrong.

Density: I still can't get over how dense some Clojure code comes out.
Sometimes I feel like it's too dense, but maybe it just takes a while to get
used to.  It does seem beginner-unfriendly.  I would be nervous to be on a team
of developers and have a new person join who's unfamiliar with Clojure.
Perhaps that just reflects my immaturity as a developer.


1/4/2015
List parsing: Seemed to get basic list parsing working; previously written
tests pass.  Pairing on 12/31 (really 12/30) cleaned up the recursive function
enough that setting up the list-parsing recursive function with a
similar-but-slightly-different recursive dispatch was mostly simple.  I spent
about 10 minutes getting an almost-working implementation, then 1-2 hours
debugging why it was returning with the calls I expected, and then some.

Recursion debugging: debugging recursive functions is kind of a pain, as the
built-in stack traces don't provide much to distinguish separate method calls.
An idea that simply came to me while working on this was to add an ID print
with each method invocation.  There's a tool in the nrepl namespace for
generating UUIDs.  I was able to find it and use it in lein repl, but not in my
code.  Clojure.core/rand is not universal, but it's good enough for debugging
purposes.

Stack traces: Stack traces in Clojure are really Java stack traces, so I've
found that they almost always contain too much noise.  Usually this noise is
all clumped together as the bottom screen's worth of stack traces, so I can
just scroll to the top.  Maybe I can investigate reducing the number of pops
for the default Clojure stack trace.

Non tail-call recursion, returning multiple values: the issue that took 1-2
hours of debugging was that my list-recursive parser, which is called from the
top-level recursive parser, returned two values in a vector that is meant to be
destructured.  The recursive calls are almost tail calls, but not quite.  Some
work is done after a recursive call to destructure the partial result and
restructure it before a return.  I know that tail call recursion tends to be
easier for compilers to optimize, but I wonder if it might also be easier to
reason about.  In today's case, I had about 5 levels of recursion when I
expected 3, and all 5 levels were present in my debug logs.  Keeping track of
this all, even with the debug output, was difficult.  It probably would have
been easier with fewer calls in the stack.

One possible way to simplify this information further is to break it up into
more functions, so that each function needs only return one value and one value
truly.  Returning multiple values in a vector definitely feels like a hack
around the fact that functions can only return one value (or is there another
way to do this?).

Regular structure: I'm starting to appreciate the regular structure of lisp
code.  At multiple times when debugging, printing out intermediate results was
a straightforward operation.


12/31/2014
Worked towards refactoring the core recursive parsing function to be more
easily reusable for list (and later dictionary) parsing.  Input for a list will
contain a text stream resembling a bencoded file, but with different stopping
conditions.

The old way of parsing integers and strings expected strict Java types.
Switched some implementations to tolerate character sequences and some
implementations DRY-ed up immensely.  Programming to sequences seems like a
Clojure-esque way of doing things.

Long stack traces and a non-Lisp aware editor made development and debugging a
bit painful.  Colored stack traces might improve development, as would some
kind of vim paren/object manipulation plugin.

Pair programming today kept me from yak shaving as much as I did in the
previous days, though soloing in previous days made pairing today smooth, as I
was ramped up on my own project.

LEIN_FAST_TRAMPOLINE ENV var was getting ignored in vim.  It gets picked up in
vim a new window, now that it's been added to my bash_profile.  Quirky.


12/26/2014
String types: count errors in string lengths for test
Spent an hour debugging why a test was failing.  The implementation was where I
wanted, but the test data had a count mismatch with the actual string length.

List parsing: this seems to necessitate recursive parsing, because a list can
contain any number of lists.  I'm not used to thinking recursively.  Explored
modifying one existing (recursive) parsing function to take a stopping
condition as a parameter, but this problem requires conditional behavior as
well (return an empty string or return the remaining input stream).  Considered
modifying the return signature to include the output-in-progress and the
input-in-progress.  This seems dirty.

Would like to next examine rewrite parsing behavior to parse one token at a
time as a separate function.
